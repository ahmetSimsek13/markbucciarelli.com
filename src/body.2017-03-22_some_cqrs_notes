<section>

<h2>Some notes from working on a CQRS application.</h2>

<p>In this post, CQRS follows the style of the <a href="http://cqrs.nu/">Edument</a> presentation,
and encompasses aggregates, commands and events.</p>

<h3>Use database transactions to writes are serialized.</h3>

<p>An aggregate encapsulates transactions and includes enough data that you can
guarantee the business rules related to those transactions are satisfied.  This
requires synchronizing access to the aggregate so that only one command is
processed at a time. </p>

<p>A typical approach is the optimistic locking ``unit of work'' pattern:</p>

<ol>

<li>load<label for="sn-hydrate" class="margin-toggle sidenote-number"></label>
<input type="checkbox" id="sn-hydrate" class="margin-toggle"/>
<span class="sidenote">
Sometimes also called ``hydrating'' the aggregate.
</span>

the aggregate from it's event history,</li>

<li>apply the command to the aggregate,</li>

<li>verify no new events written for aggregate since step 1</li>

<li>if new events have been written, go 
to<label for="sn-retry" class="margin-toggle sidenote-number"></label>
<input type="checkbox" id="sn-retry" class="margin-toggle"/>
<span class="sidenote">Optionally, stop after a limited number of retries.</span>
 step 1</li>

<li>persist any new events produced by command.</li>

</ol>

<p>A simpler approach is to use your database locking to ensure serialization.</p>

<ol>

<li>begin immediate transaction<label for="sn-trx" class="margin-toggle sidenote-number"></label>
<input type="checkbox" id="sn-trx" class="margin-toggle"/>
<span class="sidenote">In SQLite3, the immediate qualifier blocks writers immediately (instead of waiting for the first update, insert or delete statement).</span></li>

<li>select event history from the DB and load the aggregate,</li>

<li>handle the command</li>

<li>write new events to DB.</li>

<li>commit</li>

</ol>

<p>You are locking the entire table, but for small systems your are
starting out with this will work fine.  And if your system grows,
you can partition aggregate data by aggregate id.</p>

<p>When designing tables to store events, think about partitioning
by aggregate ID.  Remember that an aggregate stores enough data to
guarantee a business rule, so partitioning event history by aggregate
ID will still let you keep your rules.</p>

<h3>Put the aggregate ID in the event.</h3>

<p>At first glance, it seems like wasted space.  You are already writing the
aggregate ID into it's own (indexed) column, why include it in the event body
as well?</p>

<p>The reason is that view models will span span multiple aggregates.
Typically, events are pushed to a read model and the read model
updates its state.  Because of this pattern, you need the aggregate
ID in the event body.</p>

<h3>Put the submitted time in the command.</h3>

<p>This is useful if you want to support off-line, disconnected commands.</p>

<p>Say you want a web app that someone can use when they are not
connected to the internet.  You could use local storage, save the
sequence of commands, and then process them when they come back
on-line.  If you have any business rules that depend on the time
the command was created, then you need to store that in the
command.</p>

<h3>Make your event stream rich, even if you don't use them now.</h3>

<p>Commands don't change that often.  But read models change a lot.</p>

<p>If you make your event history rich from the get go, you will have more flexibility in building read models that go back to the start of time.</p>

<p>The downside is space usage, but again if you are partitioning by
aggregate ID, and it turns out you want to reclaim space from events
you don't need, you can turn off processing for that aggregate,
swap in a new SQLite3 database file with the events deleted, and
then turn processing back on.</p>

<p>In a system that is growing and changing quickly, this is a more
flexible design.</p>

<h3>You can also hydrate read models.</h3>

<p>I like servers that are $15 a year.  The drawback is that they typically only come with 256MB of RAM.  Instead of keeping read models hanging around in memory, you can build them on demand, by processing the event history.</p>

<p>This may be a short-term strategy, because event streams can get
big fast, but it is simple to code.  You can also monitor response
times; you may be surprised at how long you can get away with this
simpler approach.</p>

</section>
