<section>

<h2>Some notes from working on a CQRS application.</h2>

<p>In this post, CQRS follows the style of the <a href="http://cqrs.nu/">Edument</a> presentation,
and encompasses aggregates, commands and events.</p>

<h3>Use database transactions to serialize writes.</h3>

<p>An aggregate encapsulates transactions and spans enough state that you can
ensure business rules are kept.  This requires synchronizing access to the
aggregate so that only one command is processed at a time. </p>

<p>A typical approach is the optimistic locking ``unit of work'' pattern:</p>

<ol>

<li>load<label for="sn-hydrate" class="margin-toggle sidenote-number"></label>
<input type="checkbox" id="sn-hydrate" class="margin-toggle"/>
<span class="sidenote">
Sometimes also called ``hydrating'' the aggregate.
</span>

the aggregate from it's event history,</li>

<li>apply the command to the aggregate,</li>

<li>verify no new events written for aggregate since step 1</li>

<li>if new events have been written, go 
to<label for="sn-retry" class="margin-toggle sidenote-number"></label>
<input type="checkbox" id="sn-retry" class="margin-toggle"/>
<span class="sidenote">Optionally, stop after a limited number of retries.</span>
 step 1</li>

<li>persist any new events produced by command.</li>

</ol>

<p>A simpler approach is to use your database locking to ensure serialization.</p>

<ol>

<li>begin immediate transaction<label for="sn-trx" class="margin-toggle sidenote-number"></label>
<input type="checkbox" id="sn-trx" class="margin-toggle"/>
<span class="sidenote">In SQLite3, the immediate qualifier blocks writers immediately (instead of waiting for the first update, insert or delete statement).</span></li>

<li>select event history from the DB and load the aggregate,</li>

<li>handle the command</li>

<li>write new events to DB.</li>

<li>commit</li>

</ol>

<p>You are locking the entire table, but for small systems your are
starting out with this will work fine.  And if your system grows,
you can partition aggregate data by aggregate id.</p>

<p>When designing your event store, think about partitioning by aggregate ID.
Remember that an aggregate stores enough data to guarantee a business rule, so
partitioning event history by aggregate ID will still let you keep your
rules.  Taking this idea to an extreme, you could have one SQLite3 database
file for each aggregate in your system.</p>

<h3>Put the aggregate ID in the event.</h3>

<p>At first glance, it seems like wasted space.  You are already writing the
aggregate ID into it's own (indexed) column, why include it in the event body
as well?</p>

<p>The reason is that view models will span span multiple aggregates.  Events
are sent to a read model and the read model updates its state.  If the event
does not include the aggregate ID in it's body, the read model now needs the
event plus the aggregate ID in the message.  Just put it in the event.</p>

<h3>Make your event stream rich, even if you don't use them now.</h3>

<p>Commands don't change that often.  But read models change a lot.</p>

<p>If you make your event history rich from the get go, you will have more flexibility in building read models that go back to the start of time.</p>

<p>The downside is that this uses more space, but if you are partitioning by
aggregate ID, and it turns out you want to reclaim space from events you don't
need, you can turn off processing for that aggregate, swap in a new SQLite3
database file with the events deleted, and then turn processing back on.</p>

<p>In a system that is growing and changing quickly, having a rich event stream
gives you more flexible in getting data out of your system.</p>

<h3>Put the submitted time in the command.</h3>

<p>This is useful if you want to support off-line, disconnected commands.</p>

<p>Say you want a web app that someone can use when they are not
connected to the internet.  You could use local storage, save the
sequence of commands, and then process them when they come back
on-line.  If you have any business rules that depend on the time
the command was created, then you need to store that in the
command.</p>

<h3>You can also hydrate read models.</h3>

<p>I like servers that are $15 a year.  The drawback is that they typically only come with 256MB of RAM.  Instead of keeping read models hanging around in memory, you can build them on demand, by processing the event history.</p>

<p>This is a short-term strategy, because event streams get big fast, but it is
simple to code and simple to get started with.  You can also monitor response
times and disk usage; you may be surprised at how long you can get away with
this simpler approach.</p>

</section>
